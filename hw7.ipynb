{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c790de75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eca5e1-717f-451f-b23f-ef411de8576f",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 7: Word embeddings and topic modeling \n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5cb55-0576-4596-ba0e-88d0ad7c39f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b799e3-3d4e-4151-9123-9d3733a63ac3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da966a1-1d24-42a8-b959-e42202e6b824",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points}\n",
    "\n",
    "**Please be aware that this homework assignment requires installation of several packages in your course environment. It's possible that you'll encounter installation challenges, which might be frustrating. However, remember that solving these issues is not wasting time but it is an essential skill for anyone aspiring to work in data science or machine learning.**\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb. If the pdf or html also fail to render on Gradescope, please create two files for your homework: hw6a.ipynb with Exercise 1 and hw6b.ipynb with Exercises 2 and 3 and submit these two files in your submission.  \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 18, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings work well on a variety of text classification tasks. These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads the word vectors trained on Wikipedia using an algorithm called Glove. You'll need `gensim` package in your cpsc330 conda environment to run the code below. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in this pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea1002-2451-4008-aa83-54618c757bb3",
   "metadata": {},
   "source": [
    "Now that we have GloVe Wiki vectors loaded in `glove_wiki_vectors`, let's explore the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Come up with a list of 4 words of your choice and find similar words to these words using `glove_wiki_vectors` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fe4d0-4206-4747-9638-7782cca51d3f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6205ebad-49a9-435a-bf84-ae57d29c2068",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.8798075318336487),\n",
       " ('dogs', 0.8344309329986572),\n",
       " ('pet', 0.7449564337730408),\n",
       " ('puppy', 0.7236376404762268),\n",
       " ('horse', 0.7109654545783997),\n",
       " ('animal', 0.6817063093185425),\n",
       " ('pig', 0.655417263507843),\n",
       " ('boy', 0.6545307636260986),\n",
       " ('cats', 0.6471932530403137),\n",
       " ('rabbit', 0.6468629837036133)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59251b0-12c0-41cf-867b-3e5cb90f4d8d",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.8798074722290039),\n",
       " ('rabbit', 0.7424426078796387),\n",
       " ('cats', 0.7323004007339478),\n",
       " ('monkey', 0.7288709878921509),\n",
       " ('pet', 0.7190139889717102),\n",
       " ('dogs', 0.7163872718811035),\n",
       " ('mouse', 0.6915251016616821),\n",
       " ('puppy', 0.6800068616867065),\n",
       " ('rat', 0.6641027331352234),\n",
       " ('spider', 0.650113582611084)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23a9bd11-2a14-44f5-8093-b0a696324ce1",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chocolate', 0.7293441891670227),\n",
       " ('brownies', 0.6957851052284241),\n",
       " ('custard', 0.6954904198646545),\n",
       " ('caramel', 0.6932608485221863),\n",
       " ('cookies', 0.6721082329750061),\n",
       " ('cake', 0.6698510646820068),\n",
       " ('pie', 0.6569927334785461),\n",
       " ('pecan', 0.6563494205474854),\n",
       " ('dessert', 0.6547815799713135),\n",
       " ('cream', 0.6479873657226562)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"cheesecake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b922b29-e591-41a7-b82c-9bcc794d0f10",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tea', 0.7732690572738647),\n",
       " ('drinks', 0.7287518978118896),\n",
       " ('beer', 0.7253386378288269),\n",
       " ('cocoa', 0.7026590704917908),\n",
       " ('wine', 0.7002726793289185),\n",
       " ('drink', 0.6990923881530762),\n",
       " ('corn', 0.6825441718101501),\n",
       " ('sugar', 0.6775091886520386),\n",
       " ('bread', 0.6727856397628784),\n",
       " ('fruit', 0.667149007320404)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1caa0fc7-900a-4ef5-817f-166cafc8cd27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pu-erh', 0.6054583191871643),\n",
       " ('temi', 0.590826690196991),\n",
       " ('rooibos', 0.581093966960907),\n",
       " ('lapsang', 0.5794644355773926),\n",
       " ('chanoyu', 0.5764945149421692),\n",
       " ('tarnee', 0.5737316608428955),\n",
       " ('datil', 0.5632052421569824),\n",
       " ('catechins', 0.5601311922073364),\n",
       " ('jewelweed', 0.5601010322570801),\n",
       " ('kirtle', 0.5553528666496277)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.most_similar(\"matcha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24714da4-3ee5-424e-9a21-bd4b33de2e08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461c1d5-42ff-4267-bf04-e3642c5b40bb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of `glove_wiki_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b37d06f-81c7-4120-8dc7-2270105d5ecb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427682-97e3-4fa2-b2d6-84940fce9e27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c8c6c4-7113-4cb2-98f8-d34ec1c632e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between coast and shore = 0.700\n",
      "Cosine similarity between clothes and closet = 0.546\n",
      "Cosine similarity between old and new = 0.643\n",
      "Cosine similarity between smart and intelligent = 0.755\n",
      "Cosine similarity between dog and cat = 0.880\n",
      "Cosine similarity between tree and lawyer = 0.077\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2) in word_pairs:\n",
    "    print(\n",
    "        \"Cosine similarity between %s and %s = %0.3f\"\n",
    "        % (w1, w2, glove_wiki_vectors.similarity(w1, w2))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a6420-ae43-4469-99e1-59a966238a43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9186e-ab44-43e5-8a96-7a1c4130b598",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Representation of all words in English\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. The `test_words` list below contains a few new words (called neologisms) and biomedical domain-specific abbreviations. Write code to check whether `glove_wiki_vectors` have representation for these words or not. \n",
    "> If a given word `word` is in the vocabulary, `word in glove_wiki_vectors` will return True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6a6488-5139-43cb-a88c-b1d45df700cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "test_words = [\n",
    "    \"covididiot\",\n",
    "    \"fomo\",\n",
    "    \"frenemies\",\n",
    "    \"anthropause\",\n",
    "    \"photobomb\",\n",
    "    \"selfie\",\n",
    "    \"pxg\",  # Abbreviation for pseudoexfoliative glaucoma\n",
    "    \"pacg\",  # Abbreviation for primary angle closure glaucoma\n",
    "    \"cct\",  # Abbreviation for central corneal thickness\n",
    "    \"escc\",  # Abbreviation for esophageal squamous cell carcinoma\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfef3be-698f-4792-b7c3-46fac945e7f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44097c5d-97a0-450e-af88-73c9a4c90c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covididiot is NOT represented in glove_wiki_vectors\n",
      "fomo is NOT represented in glove_wiki_vectors\n",
      "frenemies is represented in glove_wiki_vectors\n",
      "anthropause is NOT represented in glove_wiki_vectors\n",
      "photobomb is NOT represented in glove_wiki_vectors\n",
      "selfie is NOT represented in glove_wiki_vectors\n",
      "pxg is NOT represented in glove_wiki_vectors\n",
      "pacg is NOT represented in glove_wiki_vectors\n",
      "cct is represented in glove_wiki_vectors\n",
      "escc is represented in glove_wiki_vectors\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    if word in glove_wiki_vectors:\n",
    "        print(\"%s is represented in glove_wiki_vectors\" %(word))\n",
    "    else:\n",
    "        print(\"%s is NOT represented in glove_wiki_vectors\" %(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2155dbe-979f-4e4d-bd4c-04d2a5f0be20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea8f10-8bfb-4698-b76c-60f5f8256d5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 Stereotypes and biases in embeddings\n",
    "rubric={points}\n",
    "\n",
    "Word vectors contain lots of useful information. But they also contain stereotypes and biases of the texts they were trained on. In the lecture, we saw an example of gender bias in Google News word embeddings. Here we are using pre-trained embeddings trained on Wikipedia data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore whether there are any worrisome biases or stereotypes present in these embeddings by trying out at least 4 examples. You can use the following two methods or other methods of your choice to explore this. \n",
    "    - the `analogy` function below which gives word analogies (an example shown below)\n",
    "    - [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods (an example is shown below)\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a90c0cd-7cf2-4f82-a617-9a87c526281d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaeec14-64c1-4226-b60f-849489077ddd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examples of using analogy to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e914a9fa-e1e9-4182-a82d-41737020e44f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.773523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.718943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.682433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.675068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.672603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>0.664246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.652045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.645348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.639333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.638750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0        nurse  0.773523\n",
       "1    physician  0.718943\n",
       "2      doctors  0.682433\n",
       "3      patient  0.675068\n",
       "4      dentist  0.672603\n",
       "5     pregnant  0.664246\n",
       "6      medical  0.652045\n",
       "7      nursing  0.645348\n",
       "8       mother  0.639333\n",
       "9     hospital  0.638750"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fceb0128-3c9b-4674-ae40-da3d80f3f00c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14283244"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"aboriginal\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774cb08e-95bc-458b-bc78-ee3fcf2b2f7a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3518238"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bef6d-9a57-41a4-8a83-a65d7bc07783",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a227771e-f8b7-4bad-9881-eb5528f10e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : intelligent :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>articulate</td>\n",
       "      <td>0.627857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thoughtful</td>\n",
       "      <td>0.616820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smart</td>\n",
       "      <td>0.587221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strong-willed</td>\n",
       "      <td>0.568862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vivacious</td>\n",
       "      <td>0.550627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>opinionated</td>\n",
       "      <td>0.547024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>perceptive</td>\n",
       "      <td>0.545705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inquisitive</td>\n",
       "      <td>0.544539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>energetic</td>\n",
       "      <td>0.533960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>literate</td>\n",
       "      <td>0.533894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0     articulate  0.627857\n",
       "1     thoughtful  0.616820\n",
       "2          smart  0.587221\n",
       "3  strong-willed  0.568862\n",
       "4      vivacious  0.550627\n",
       "5    opinionated  0.547024\n",
       "6     perceptive  0.545705\n",
       "7    inquisitive  0.544539\n",
       "8      energetic  0.533960\n",
       "9       literate  0.533894"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"intelligent\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6025b7c9-5ffd-4076-8d44-0440d01efe69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : gentle :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sensuous</td>\n",
       "      <td>0.601437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lovely</td>\n",
       "      <td>0.589904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sassy</td>\n",
       "      <td>0.580544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>playful</td>\n",
       "      <td>0.580543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sultry</td>\n",
       "      <td>0.566077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>soothing</td>\n",
       "      <td>0.564392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>breezy</td>\n",
       "      <td>0.560906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>demure</td>\n",
       "      <td>0.560848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vivacious</td>\n",
       "      <td>0.553936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>refreshing</td>\n",
       "      <td>0.550070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0     sensuous  0.601437\n",
       "1       lovely  0.589904\n",
       "2        sassy  0.580544\n",
       "3      playful  0.580543\n",
       "4       sultry  0.566077\n",
       "5     soothing  0.564392\n",
       "6       breezy  0.560906\n",
       "7       demure  0.560848\n",
       "8    vivacious  0.553936\n",
       "9   refreshing  0.550070"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"gentle\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deaa74e9-858e-4b28-a7a8-6b6b522c1ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5252884, 0.5490399)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"woman\", \"beautiful\"), glove_wiki_vectors.similarity(\n",
    "    \"man\", \"beautiful\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e306e7-26f3-4367-8935-0be49b86be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33403102, 0.4299851)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"woman\", \"engineer\"), glove_wiki_vectors.similarity(\n",
    "    \"man\", \"engineer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474b6ac-25f3-45f1-97db-bfadf71d9f80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Discussion\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Discuss your observations from 1.4. Are there any worrisome biases in these embeddings trained on Wikipedia?   \n",
    "2. Give an example of how using embeddings with biases could cause harm in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e95ea8-80c0-4184-8b89-1fa1581404f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c125c3",
   "metadata": {},
   "source": [
    "1. These embeddings appear to exhibit less bias compared to the GoogleNews pre-trained embeddings. This improvement can be attributed to modern methods for computing word embeddings, which incorporate debiasing techniques.\n",
    "2. Using biased embeddings in real-world applications can have significant consequences. For example, an AI-powered hiring tool trained on biased embeddings might rank male candidates higher for roles like engineer or doctor while favoring female candidates for nursing or caregiving roles, perpetuating workplace inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb13f7-e08c-4f8d-86c9-b1602cc8a5b4",
   "metadata": {},
   "source": [
    "## Exercise 2: Topic modeling \n",
    "\n",
    "The goal of topic modeling is discovering high-level themes in a large collection of texts. \n",
    "\n",
    "In this homework, you will explore topics in [the 20 newsgroups text dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) using `scikit-learn`'s `LatentDirichletAllocation` (LDA) model. \n",
    "\n",
    "Usually, topic modeling is used for discovering abstract \"topics\" that occur in a collection of documents when you do not know the actual topics present in the documents. But 20 newsgroups text dataset is labeled with categories (e.g., sports, hardware, religion), and you will be able to cross-check the topics discovered by your model with these available topics. \n",
    "\n",
    "The starter code below loads the train and test portion of the data and convert the train portion into a pandas DataFrame. For speed, we will only consider documents with the following 8 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "327ca91e-cf57-4481-b4cc-46c29a9849a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9afa3b30-7f32-48ec-8291-69c964a38c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name  \n",
       "0        talk.politics.guns  \n",
       "1             comp.graphics  \n",
       "2             comp.graphics  \n",
       "3            comp.windows.x  \n",
       "4     talk.politics.mideast  \n",
       "...                     ...  \n",
       "4558          comp.graphics  \n",
       "4559         comp.windows.x  \n",
       "4560     talk.politics.guns  \n",
       "4561         comp.windows.x  \n",
       "4562  talk.politics.mideast  \n",
       "\n",
       "[4563 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = [\n",
    "    \"rec.sport.hockey\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.windows.x\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.guns\",\n",
    "]  # We'll only consider these categories out of 20 categories for speed.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=cats\n",
    ")\n",
    "X_news_train, y_news_train = newsgroups_train.data, newsgroups_train.target\n",
    "df = pd.DataFrame(X_news_train, columns=[\"text\"])\n",
    "df[\"target\"] = y_news_train\n",
    "df[\"target_name\"] = [\n",
    "    newsgroups_train.target_names[target] for target in newsgroups_train.target\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8095c853-d4d5-49a3-bda0-129ffd2f690b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cdcfc-7545-4a40-a9b3-34b58bb38365",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21b066-8de6-42a7-8e4e-097fd8727367",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Preprocessing using [spaCy](https://spacy.io/)\n",
    "rubric={points}\n",
    "\n",
    "Preprocessing is a crucial step before carrying out topic modeling and it markedly affects topic modeling results. In this exercise, you'll prepare the data using [spaCy](https://spacy.io/) for topic modeling. \n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "- Write code using [spaCy](https://spacy.io/) to preprocess the `text` column in the given dataframe `df` and save the processed text in a new column called `text_pp` within the same dataframe.\n",
    "\n",
    "If you do not have spaCy in your course environment, you'll have to [install it](https://spacy.io/usage) and download the pretrained model en_core_web_md. \n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "\n",
    "Note that there is no such thing as \"perfect\" preprocessing. You'll have to make your own judgments and decisions on which tokens are likely to be more informative for the given task. Some common text preprocessing steps for topic modeling include: \n",
    "- getting rid of slashes, new-line characters, or any other non-informative characters\n",
    "- sentence segmentation and tokenization      \n",
    "- replacing urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\". \n",
    "- getting rid of other fairly unique tokens which are not going to help us in topic modeling  \n",
    "- excluding stopwords and punctuation \n",
    "- lemmatization\n",
    "\n",
    "\n",
    "> Check out [these available attributes](https://spacy.io/api/token#attributes) for `token` in spaCy which might help you with preprocessing. \n",
    "\n",
    "> You can also get rid of words with specific POS tags. [Here](https://universaldependencies.org/u/pos/) is the list of part-of-speech tags used in spaCy. \n",
    "\n",
    "> You may have to use regex to clean text before passing it to spaCy. Also, you might have to go back and forth between preprocessing in this exercise and and topic modeling in Exercise 2 before finalizing preprocessing steps. \n",
    "\n",
    "> Note that preprocessing the corpus might take some time. So here are a couple of suggestions: 1) During the debugging phase, work on a smaller subset of the data. 2) Once you finalize the preprocessing part, you might want to save the preprocessed data in a CSV and work with this CSV so that you don't run the preprocessing part every time you run the notebook. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6816e7a3-d6d4-4bef-81f4-9b4fdf638175",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036dbe6-2353-4c5a-80bf-2b884a170a29",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8be7c802-d10b-4929-9664-34274299e884",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi,\\n   I'd like to subscribe to Leadership Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nVery simple.\\n\\n\"X-Soviet Armenian governm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nNo no no!!!  It's a squid!  Keep the traditi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: Center for Policy Research &lt;cpr&gt;\\nSubjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n  The OpenLook window manager source is avai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\n\\n\\nTwo questions:\\n\\n1)  You asserted that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The motif mailing list will now be located at\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\\n\\n\\nAre you suggesting that we should forget...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>There have been a number of articles on the PB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\nHmm.  I beg to differ.  It will probably mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hi,\\n\\nI wonder if it is possible for a parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\n\\nReally&gt;`?\\n\\n\\nNo, gravity is an inherent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>G-d has nothing to do with it. Some of the lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hi, \\n\\n\\tIs anyone into medical imaging?  I h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\n\\nThe quotation marks should enclose \"laws,\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   You know, I was reading 18 U.S.C. 922 and some...\n",
       "1   \\n\\n\\nIt's not a bad question: I don't have an...\n",
       "2   \\nActuallay I don't, but on the other hand I d...\n",
       "3   The following problem is really bugging me,\\na...\n",
       "4   \\n\\n  This is the latest from UPI \\n\\n     For...\n",
       "5   Hi,\\n   I'd like to subscribe to Leadership Ma...\n",
       "6   \\n\\nVery simple.\\n\\n\"X-Soviet Armenian governm...\n",
       "7   \\nNo no no!!!  It's a squid!  Keep the traditi...\n",
       "8   From: Center for Policy Research <cpr>\\nSubjec...\n",
       "9   \\n  The OpenLook window manager source is avai...\n",
       "10  \\n\\n\\nTwo questions:\\n\\n1)  You asserted that ...\n",
       "11  The motif mailing list will now be located at\\...\n",
       "12  \\n\\n\\nAre you suggesting that we should forget...\n",
       "13  There have been a number of articles on the PB...\n",
       "14  \\nHmm.  I beg to differ.  It will probably mak...\n",
       "15  Hi,\\n\\nI wonder if it is possible for a parent...\n",
       "16  \\n\\nReally>`?\\n\\n\\nNo, gravity is an inherent ...\n",
       "17  G-d has nothing to do with it. Some of the lan...\n",
       "18  Hi, \\n\\n\\tIs anyone into medical imaging?  I h...\n",
       "19  \\n\\nThe quotation marks should enclose \"laws,\"..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df = pd.DataFrame(df[:20])\n",
    "pd.DataFrame(dummy_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42b1b667-2a6b-46ef-80ea-8a75d671397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref from lecture 18 \n",
    "\n",
    "def preprocess(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # print(\"Named entities:\\n\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "    clean_text = []\n",
    "    tokens = [token for token in doc]  \n",
    "    # print(\"\\nTokens: \"analogy, tokens)\n",
    "\n",
    "    # Accessing lemma\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # print(\"\\nLemmas: \", lemmas)\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "        if token.like_num:\n",
    "            clean_text.append(\"NUM\")\n",
    "\n",
    "    # displacy.render(doc, style=\"ent\")\n",
    "\n",
    "    # print(\"Named entities:\\n\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a68098c0-8555-4924-8230-0cf0c776daea",
   "metadata": {},
   "outputs": [],
   "source": [
    " # [ preprocess(text) for text in nlp.pipe(dummy_df['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2689898-8815-449c-9087-1690804482c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(dummy_df['text'])]\n",
    "df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(df['text'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "946e3392-afbc-4b3c-a6c4-e1af89a5f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_df[\"named_ents\"] = [preprocess(doc) for doc in nlp.pipe(dummy_df[\"text\"], batch_size=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8d896-53f3-4b96-a7b4-e61dc4345693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47078962-3442-4b50-b781-b0463e69a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy import displacy\n",
    "\n",
    "\n",
    "\n",
    "# displacy.render(doc, style=\"ent\")\n",
    "# # Text and label of named entity span\n",
    "# print(\"Named entities:\\n\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "# print(\"\\nORG means: \", spacy.explain(\"ORG\"))\n",
    "# print(\"GPE means: \", spacy.explain(\"GPE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba8da0d6-f98b-41d7-a00d-8ef38b7bc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dummy_df['text'][0])\n",
    "# print(dummy_df['text_pp'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba75d96d-01dc-4b3b-b222-67c63b5fd0c5",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comment out when using original df \n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vec = CountVectorizer(stop_words=\"english\")\n",
    "# X = vec.fit_transform(dummy_df[\"text_pp\"])\n",
    "# X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87727070-56e6-4491-a4db-2bc10753e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_df.iloc[2:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c2b633b-6601-4b83-8668-800a99d3ba20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>know read NUM u.s.c. 922 NUM sence wonder help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>bad question ref list algorithm think bit hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay hand support idea have NUM newsgroup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>follow problem bug appreciate help create NUM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>look software call shadow know simple raytrace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>archive faq part3 modify 1993/04/04 ----------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>nice answer question difference fed mandate li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>get gateway 4dx-33v try configure need correct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>armenians nagarno karabagh defend right homela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name                                            text_pp  \n",
       "0        talk.politics.guns  know read NUM u.s.c. 922 NUM sence wonder help...  \n",
       "1             comp.graphics  bad question ref list algorithm think bit hard...  \n",
       "2             comp.graphics  actuallay hand support idea have NUM newsgroup...  \n",
       "3            comp.windows.x  follow problem bug appreciate help create NUM ...  \n",
       "4     talk.politics.mideast  late upi foreign ministry spokesman ferhat ata...  \n",
       "...                     ...                                                ...  \n",
       "4558          comp.graphics  look software call shadow know simple raytrace...  \n",
       "4559         comp.windows.x  archive faq part3 modify 1993/04/04 ----------...  \n",
       "4560     talk.politics.guns  nice answer question difference fed mandate li...  \n",
       "4561         comp.windows.x  get gateway 4dx-33v try configure need correct...  \n",
       "4562  talk.politics.mideast  armenians nagarno karabagh defend right homela...  \n",
       "\n",
       "[4563 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "deb59bfc-1b27-4873-90d9-102662842ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0579dd3-40cd-438b-bbfb-cffb187ee5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d358be1-c1c1-40bd-a9bf-7a869bf3bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89228a-2c70-4999-b430-77d79808248d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94bbad26-84c1-41ed-8201-0843924b3166",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f9bd5-31ef-43dd-9ec7-e0e1cd9a9714",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Justification\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Outline the preprocessing steps you carried out in the previous exercise (bullet point format is fine), providing a brief justification when necessary. \n",
    "\n",
    "> You might want to wait to answer this question till you are done with Exercise 2 and you have finalized the preprocessing steps in 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8df885-0fb7-4ec7-bef6-bb0c3cd82e9e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a06bf7",
   "metadata": {},
   "source": [
    "* Lowercasing - to make words consistent for processing. i.e. 'name' and 'Name' are the same\n",
    "* removing stopwords - Remove really common english words that won't contribute to topic modeling due to being so commonly used. (i.e. the, and, is)\n",
    "* POS filtering - remove additional filler words like propositions, pronouns, etc that don't have much relevance to the overall meaning of a document. \n",
    "* Replace numbers with 'NUM' so that they don't impact topic modeling process. i.e. we don't want a category of just numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22f0e-2526-4f83-94ee-a625c7a5ed04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080f91c-6801-4b89-9fbc-1b574b09915c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Build a topic model using sklearn's LatentDirichletAllocation\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Build LDA models on the preprocessed data using using [sklearn's `LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and random state 42. Experiment with a few values for the number of topics (`n_components`). Pick a reasonable number for the number of topics and briefly justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e472711-3c92-4b82-a31d-5d4bf55dbf2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5a649",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88908be8-85df-40bf-9d6d-0b48948a7137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vec = CountVectorizer(stop_words=\"english\")\n",
    "# X = vec.fit_transform(dummy_df[\"text_pp\"])\n",
    "# X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52d22809-6212-4677-a255-8468b63cfbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4563x39306 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 275024 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(stop_words=\"english\")\n",
    "X = vec.fit_transform(df[\"text_pp\"])\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bed16e6f-2706-463e-80cc-c64d2b75a296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.63196995e+02, 1.73439615e+02, 1.25110310e-01, ...,\n",
       "        1.25000000e-01, 1.25058070e-01, 1.25433862e-01],\n",
       "       [1.09379366e+01, 5.42016867e+01, 2.12250691e+00, ...,\n",
       "        1.25000000e-01, 1.25000001e-01, 1.25000004e-01],\n",
       "       [1.25186953e-01, 5.67560126e+01, 1.26443806e-01, ...,\n",
       "        1.25000000e-01, 1.25078245e-01, 1.25000006e-01],\n",
       "       ...,\n",
       "       [1.13326005e+00, 1.25054828e-01, 1.25000010e-01, ...,\n",
       "        1.12500000e+00, 1.12478302e+00, 1.25000032e-01],\n",
       "       [2.38046978e+00, 1.08089941e+02, 1.25161170e-01, ...,\n",
       "        1.25000000e-01, 1.25000002e-01, 1.12443973e+00],\n",
       "       [4.37481816e+00, 3.41662228e+00, 1.25758889e-01, ...,\n",
       "        1.25000000e-01, 1.25000001e-01, 1.25126331e-01]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 8\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=42\n",
    ")\n",
    "lda.fit(X) \n",
    "document_topics = lda.transform(X)\n",
    "\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dc08628-434b-41f0-b7bf-c15b477bb2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39306\n",
      "lda.components_.shape: (8, 39306)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(vec.get_feature_names_out()))\n",
    "\n",
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20067136-7d1b-46c4-984e-c3acb0bbb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4ba4428-addb-4f80-b929-94bb60993d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', ..., 'zz', 'zzzzzz', 'zzzzzzt'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(vec.get_feature_names_out())\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19e77310-f677-4fef-b25a-8a8c50f40f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       \n",
      "--------      --------      --------      --------      \n",
      "num           num           num           jews          \n",
      "game          file          say           num           \n",
      "team          image         people        israeli       \n",
      "play          edu           know          people        \n",
      "year          window        come          right         \n",
      "player        use           think         israel        \n",
      "win           program       time          think         \n",
      "season        available     kill          state         \n",
      "good          server        tell          jewish        \n",
      "hockey        run           armenians     point         \n",
      "league        display       armenian      claim         \n",
      "period        include       want          nazi          \n",
      "55            work          israel        war           \n",
      "new           widget        happen        know          \n",
      "run           version       start         human         \n",
      "\n",
      "\n",
      "topic 4       topic 5       topic 6       topic 7       \n",
      "--------      --------      --------      --------      \n",
      "num           libxmu        gun           num           \n",
      "entry         captain       num           god           \n",
      "file          xmu           law           know          \n",
      "output        adam          people        think         \n",
      "program       trade         weapon        people        \n",
      "section       edu           state         believe       \n",
      "line          doug          right         jesus         \n",
      "char          let           firearm       say           \n",
      "rule          player        government    time          \n",
      "build         symbol        crime         good          \n",
      "use           league        control       thing         \n",
      "int           hockey        think         question      \n",
      "oname         gainey        greek         come          \n",
      "info          bob           turkish       way           \n",
      "null          undefine      criminal      church        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(8),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=4,\n",
    "    n_words=15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "396ce350-c231-4099-9a6f-9df19ecd6888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23754225, 0.00208622, 0.00208723, 0.11535271, 0.00208456,\n",
       "       0.00208336, 0.48630431, 0.15245937])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20227b11-4ce3-489b-8593-966f92dd42dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45dcebf1-ac1b-4715-874a-43c9aa11ca1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'know read NUM u.s.c. 922 NUM sence wonder help u.s.c. 922 NUM NUM provide paragraph NUM shall unlawful person transfer possess machinegun get look law dictionary find person artificial entity create government right federal constitution understand statute 922 NUM enforce individual tell government tell possess pass law law know law constitional go court go court run mill guilty jail claim right possess tell supreme court right'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['text_pp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "453ff02b-cc6d-44a0-adb7-4c2904811496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad question ref list algorithm think bit hard NUM NUM point find plane define circle lie algorithm NUM find center circle line pass center perpendicular plane NUM point pass center sphere NUM repeat unused point NUM original point give NUM different line pass sphere origin interection center sphere NUM radius easy compute distance center original point leave math workable algorithm alternate method pair point plane form perpendicular bisector line segment pair contain center sphere NUM pair form NUM plane intersect point easy implement'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1]['text_pp'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "644f35e2-9691-4d82-8466-731e7916fd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00154642, 0.82367806, 0.00154645, 0.1670488 , 0.0015452 ,\n",
       "       0.00154336, 0.00154575, 0.00154596])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6520b28-371e-4cb5-9291-c514df1f4146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nActuallay I don't, but on the other hand I don't support the idea of having\\none newsgroup for every aspect of graphics programming as proposed by Brian,\\nin his reply to my original posting.\\nI would suggest a looser structure more like a comp.graphics.programmer,\\ncomp.graphics.hw_and_sw\\nThe reason for making as few groups as possible is for the same reason you\\nsay we shouldn't spilt up, not to get to few postings every day.\\nI takes to much time to browse through all postings just to find two or \\nthree I'm interested in.\\n\\nI understand and agree when you say you want all aspects of graphics in one\\nmeeting. I agree to some extension. I see news as a forum to exchange ideas,\\nhelp others or to be helped. I think this is difficult to achive if there\\nare so many different things in one meeting.\\n\\nGood evening netters|-)\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be7b7a1a-b231-413d-9021-8ccf613e311e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(document_topics[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1afbdb-b419-4d17-a7fa-0dfd9e618f15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4ff43-188c-4d9a-8404-691cd563dd9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.4 Exploring word topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. For the number of topics you picked in the previous exercise, show top 10 words for each of your topics and suggest labels for each of the topics (similar to how we came up with labels \"health and nutrition\", \"fashion\", and \"machine learning\" in the toy example we saw in class). \n",
    "\n",
    "> If your topics do not make much sense, you might have to go back to preprocessing in Exercise 2.1, improve it, and train your LDA model again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbe9de-c9d9-41a1-bf5a-10220ce6fa38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "967136f3-5746-4568-8d2d-d48c4d3be36c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: num, game, team, play, year, player, win, season, good, hockey\n",
      "Topic 1: num, file, image, edu, window, use, program, available, server, run\n",
      "Topic 2: num, say, people, know, come, think, time, kill, tell, armenians\n",
      "Topic 3: jews, num, israeli, people, right, israel, think, state, jewish, point\n",
      "Topic 4: num, entry, file, output, program, section, line, char, rule, build\n",
      "Topic 5: libxmu, captain, xmu, adam, trade, edu, doug, let, player, symbol\n",
      "Topic 6: gun, num, law, people, weapon, state, right, firearm, government, crime\n",
      "Topic 7: num, god, know, think, people, believe, jesus, say, time, good\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]\n",
    "    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18caef39-4161-438b-90f2-99337f39dcd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_labels = {\n",
    "    0: \"Hockey + Sports\",\n",
    "    1: \"Software Systems\",\n",
    "    2: \"Armenian Genocide/Conflict\",\n",
    "    3: \"Israel and Jewish Discussion\",\n",
    "    4: \"Programming\",\n",
    "    5: \"Software Libraries\",\n",
    "    6: \"Politics\",\n",
    "    7: \"Religion\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79ccb653-772d-4f24-81f3-783f21833388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Hockey + Sports\n",
      "Topic 1: Software Systems\n",
      "Topic 2: Armenian Genocide/Conflict\n",
      "Topic 3: Israel and Jewish Discussion\n",
      "Topic 4: Programming\n",
      "Topic 5: Software Libraries\n",
      "Topic 6: Politics\n",
      "Topic 7: Religion\n"
     ]
    }
   ],
   "source": [
    "# Display assigned labels for verification\n",
    "for topic_idx, label in topic_labels.items():\n",
    "    print(f\"Topic {topic_idx}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a63b3701-205a-4611-8056-fc66e2705dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>know read NUM u.s.c. 922 NUM sence wonder help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>bad question ref list algorithm think bit hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay hand support idea have NUM newsgroup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>follow problem bug appreciate help create NUM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>look software call shadow know simple raytrace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>archive faq part3 modify 1993/04/04 ----------...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>nice answer question difference fed mandate li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>get gateway 4dx-33v try configure need correct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>armenians nagarno karabagh defend right homela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name                                            text_pp  \n",
       "0        talk.politics.guns  know read NUM u.s.c. 922 NUM sence wonder help...  \n",
       "1             comp.graphics  bad question ref list algorithm think bit hard...  \n",
       "2             comp.graphics  actuallay hand support idea have NUM newsgroup...  \n",
       "3            comp.windows.x  follow problem bug appreciate help create NUM ...  \n",
       "4     talk.politics.mideast  late upi foreign ministry spokesman ferhat ata...  \n",
       "...                     ...                                                ...  \n",
       "4558          comp.graphics  look software call shadow know simple raytrace...  \n",
       "4559         comp.windows.x  archive faq part3 modify 1993/04/04 ----------...  \n",
       "4560     talk.politics.guns  nice answer question difference fed mandate li...  \n",
       "4561         comp.windows.x  get gateway 4dx-33v try configure need correct...  \n",
       "4562  talk.politics.mideast  armenians nagarno karabagh defend right homela...  \n",
       "\n",
       "[4563 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fc6bfa2-b390-42ca-85dc-333fcb2598e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hockey + Sports</th>\n",
       "      <th>Software Systems</th>\n",
       "      <th>Armenian Genocide/Conflict</th>\n",
       "      <th>Israel and Jewish Discussion</th>\n",
       "      <th>Programming</th>\n",
       "      <th>Software Libraries</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.237542</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.115353</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.486304</td>\n",
       "      <td>0.152459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.823678</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.167049</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.001546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.280636</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.706849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.384478</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.590508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.046909</td>\n",
       "      <td>0.935636</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.002909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.620765</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.354215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.999840</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.183337</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.086464</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.717164</td>\n",
       "      <td>0.002610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.776198</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.205040</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.003127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.422607</td>\n",
       "      <td>0.494867</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.080611</td>\n",
       "      <td>0.000383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hockey + Sports  Software Systems  Armenian Genocide/Conflict  \\\n",
       "0            0.237542          0.002086                    0.002087   \n",
       "1            0.001546          0.823678                    0.001546   \n",
       "2            0.002087          0.280636                    0.002087   \n",
       "3            0.004169          0.384478                    0.004171   \n",
       "4            0.002908          0.046909                    0.935636   \n",
       "...               ...               ...                         ...   \n",
       "4558         0.004168          0.620765                    0.004174   \n",
       "4559         0.000023          0.999840                    0.000023   \n",
       "4560         0.002605          0.002606                    0.183337   \n",
       "4561         0.003128          0.776198                    0.003129   \n",
       "4562         0.000383          0.422607                    0.494867   \n",
       "\n",
       "      Israel and Jewish Discussion  Programming  Software Libraries  Politics  \\\n",
       "0                         0.115353     0.002085            0.002083  0.486304   \n",
       "1                         0.167049     0.001545            0.001543  0.001546   \n",
       "2                         0.002087     0.002085            0.002084  0.002086   \n",
       "3                         0.004169     0.004170            0.004167  0.004168   \n",
       "4                         0.002911     0.002909            0.002907  0.002911   \n",
       "...                            ...          ...                 ...       ...   \n",
       "4558                      0.004171     0.004171            0.004167  0.004169   \n",
       "4559                      0.000023     0.000023            0.000023  0.000023   \n",
       "4560                      0.002609     0.086464            0.002605  0.717164   \n",
       "4561                      0.003126     0.205040            0.003126  0.003127   \n",
       "4562                      0.000383     0.000384            0.000382  0.080611   \n",
       "\n",
       "      Religion  \n",
       "0     0.152459  \n",
       "1     0.001546  \n",
       "2     0.706849  \n",
       "3     0.590508  \n",
       "4     0.002909  \n",
       "...        ...  \n",
       "4558  0.354215  \n",
       "4559  0.000023  \n",
       "4560  0.002610  \n",
       "4561  0.003127  \n",
       "4562  0.000383  \n",
       "\n",
       "[4563 rows x 8 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_probabilities = pd.DataFrame(document_topics, columns=topic_labels)\n",
    "topic_probabilities = topic_probabilities.rename(columns=topic_labels)\n",
    "topic_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77cc678e-098c-4de7-a6d1-38ecdf2f2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([topic_probabilities, df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cbbb9-1057-43ef-ac63-842e3e4cb43c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53117b-897c-4eba-b1bd-dfedcb3d35ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.5 Exploring document topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Show the document topic assignment of the first five documents from `df`. \n",
    "2. Comment on the document topic assignment of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12934a67-c48d-4c42-a856-248615c6202b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fdf252c7-1a18-4555-a244-9b7a75f63db9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hockey + Sports</th>\n",
       "      <th>Software Systems</th>\n",
       "      <th>Armenian Genocide/Conflict</th>\n",
       "      <th>Israel and Jewish Discussion</th>\n",
       "      <th>Programming</th>\n",
       "      <th>Software Libraries</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Religion</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.237542</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.115353</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.486304</td>\n",
       "      <td>0.152459</td>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>know read NUM u.s.c. 922 NUM sence wonder help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.823678</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.167049</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>bad question ref list algorithm think bit hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.280636</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.706849</td>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay hand support idea have NUM newsgroup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.384478</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.590508</td>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>follow problem bug appreciate help create NUM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.046909</td>\n",
       "      <td>0.935636</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hockey + Sports  Software Systems  Armenian Genocide/Conflict  \\\n",
       "0         0.237542          0.002086                    0.002087   \n",
       "1         0.001546          0.823678                    0.001546   \n",
       "2         0.002087          0.280636                    0.002087   \n",
       "3         0.004169          0.384478                    0.004171   \n",
       "4         0.002908          0.046909                    0.935636   \n",
       "\n",
       "   Israel and Jewish Discussion  Programming  Software Libraries  Politics  \\\n",
       "0                      0.115353     0.002085            0.002083  0.486304   \n",
       "1                      0.167049     0.001545            0.001543  0.001546   \n",
       "2                      0.002087     0.002085            0.002084  0.002086   \n",
       "3                      0.004169     0.004170            0.004167  0.004168   \n",
       "4                      0.002911     0.002909            0.002907  0.002911   \n",
       "\n",
       "   Religion                                               text  target  \\\n",
       "0  0.152459  You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1  0.001546  \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2  0.706849  \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3  0.590508  The following problem is really bugging me,\\na...       2   \n",
       "4  0.002909  \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "\n",
       "             target_name                                            text_pp  \n",
       "0     talk.politics.guns  know read NUM u.s.c. 922 NUM sence wonder help...  \n",
       "1          comp.graphics  bad question ref list algorithm think bit hard...  \n",
       "2          comp.graphics  actuallay hand support idea have NUM newsgroup...  \n",
       "3         comp.windows.x  follow problem bug appreciate help create NUM ...  \n",
       "4  talk.politics.mideast  late upi foreign ministry spokesman ferhat ata...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92059c85-f9e7-41a3-9f63-e4f0e6353c75",
   "metadata": {},
   "source": [
    "From these first 5 rows of the document-topic probability assignments, it is clear that the LDA model assigns each document to a specific topic based on the highest probability (the predicted topic). In most cases, the probability for the most likely topic is much higher than for the other topics, which indicates that the model is confident in its assignment. However, the confidence in the predicted topic is relatively lower for row number 2 even though it is still more confident of the predicted topic than the other topics, this may suggest that the document may contain content that overlaps with multiple topics and leave room for improvement. Possible asjustments that could increase improvement are adjusting hyperparameters like max_iterations and n_components, or preprocess the text further. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e21cac-5f1a-4480-b683-47c3b41a1199",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42685c5-01cb-4495-aae6-f803d6f75172",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 3: Short answer questions \n",
    "<hr>\n",
    "\n",
    "rubric={points}\n",
    "\n",
    "1. Briefly explain how content-based filtering works in the context of recommender systems. \n",
    "2. Discuss at least two negative consequences of recommender systems.\n",
    "3. What is transfer learning in natural language processing? Briefly explain.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01807641-531e-454a-9374-fc8ad04bc30b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f9fa2",
   "metadata": {},
   "source": [
    "\n",
    "1. content-based filtering gathers information about users mainly through the items they have interacted with. They are more likely to reccomend certain item to users based on their history of likes, dislikes, and ratings with regards to the item or content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff9bfd-449d-46e4-b201-e6a9ab42c352",
   "metadata": {},
   "source": [
    "2. Collaborative filtering systems struggle with cold starts, which occur when new users or items are added to the system. For example:\n",
    "\n",
    "    New users: Without historical preferences (e.g., likes, ratings, or interactions), the system has little data to generate meaningful recommendations.\n",
    "    New items: Items without prior engagement data (e.g., reviews or ratings) cannot be linked effectively to existing users or similar items, leading to poor recommendations.\n",
    "    This limitation disrupts the system's ability to provide accurate and relevant suggestions for both new users and items.\n",
    "       Also, recommender systems often expose users to the same views, perspectives, and ideas based on their past interactions. This can lead to:\n",
    "    Filter bubbles: Users are repeatedly recommended content that aligns with their existing preferences, limiting exposure to diverse or opposing viewpoints.\n",
    "    Bias reinforcement: Particularly in sensitive areas like politics or science, recommendations may consistently reinforce pre-existing biases, deepening polarization.\n",
    "    Ethical concerns: The lack of diversity in recommendations hinders opportunities for personal growth, learning, and exploring new perspectives, raising ethical questions about the system's impact on society.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9293587-bc0d-4902-9bcd-c65b877b1ca8",
   "metadata": {},
   "source": [
    "3. Transfer learning in NLP involves using pre-trained models, such as BERT, GPT, or Word2Vec, which have been trained on large datasets to understand general language patterns, and fine-tuning them on specific tasks with new data. This approach is highly efficient because it avoids the computational expense of training models from scratch, especially when dealing with vast vocabularies like in English. These pre-trained models provide embeddings or contextual representations of language that can be applied to tasks like text classification or sentiment analysis. However, they may also inherit societal biases present in the original training data, which can impact fairness and accuracy in downstream applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929238-7f2c-421e-bb9a-32f0debf0636",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d6d11-0ef5-4315-9854-f64a427e62af",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491f1d3-dd84-4ed2-b2b3-9148ac8df7a5",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
